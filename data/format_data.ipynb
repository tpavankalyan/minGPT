{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e18f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d580c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111110\n"
     ]
    }
   ],
   "source": [
    "with open(\"/datadrive/pavan/repos/synthetic_reasoning/data/all_pairs.jsonl\") as f:\n",
    "    all_pairs = json.load(f)\n",
    "print(len(all_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8cdc9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1111110/1111110 [01:10<00:00, 15823.03it/s]\n"
     ]
    }
   ],
   "source": [
    "question = []\n",
    "answer = []\n",
    "solution = []\n",
    "algorithm = []\n",
    "length = []\n",
    "text = []\n",
    "c = 0\n",
    "for i in tqdm(range(len(all_pairs))):\n",
    "    for k in all_pairs[i]:\n",
    "        if k.endswith('_sort'):\n",
    "            q = ' '.join(map(str, all_pairs[i]['unsorted']))\n",
    "            a = ' '.join(map(str, all_pairs[i]['sorted']))\n",
    "            s = '<|T|> '+' <|T|> '.join([' '.join(map(str, step)) for step in all_pairs[i][k]])\n",
    "            algo = k.split('_')[0]\n",
    "            question.append(q)\n",
    "            answer.append(a)\n",
    "            solution.append(s)\n",
    "            algorithm.append(algo)\n",
    "            length.append(len(all_pairs[i]['sorted']))\n",
    "            text.append(\"<|Q|> \"+q+\" <|S|> \"+algo+\" \"+s+\" <|A|> \"+a+\" <|E|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34f19f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>solution</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;|T|&gt; 0</td>\n",
       "      <td>bubble</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;|Q|&gt; 0 &lt;|S|&gt; bubble &lt;|T|&gt; 0 &lt;|A|&gt; 0 &lt;|E|&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;|T|&gt; 0</td>\n",
       "      <td>selection</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;|Q|&gt; 0 &lt;|S|&gt; selection &lt;|T|&gt; 0 &lt;|A|&gt; 0 &lt;|E|&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;|T|&gt; 0</td>\n",
       "      <td>insertion</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;|Q|&gt; 0 &lt;|S|&gt; insertion &lt;|T|&gt; 0 &lt;|A|&gt; 0 &lt;|E|&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;|T|&gt; 0</td>\n",
       "      <td>quick</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;|Q|&gt; 0 &lt;|S|&gt; quick &lt;|T|&gt; 0 &lt;|A|&gt; 0 &lt;|E|&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;|T|&gt; 0</td>\n",
       "      <td>heap</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;|Q|&gt; 0 &lt;|S|&gt; heap &lt;|T|&gt; 0 &lt;|A|&gt; 0 &lt;|E|&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question answer solution  algorithm  length  \\\n",
       "0        0      0  <|T|> 0     bubble       1   \n",
       "1        0      0  <|T|> 0  selection       1   \n",
       "2        0      0  <|T|> 0  insertion       1   \n",
       "3        0      0  <|T|> 0      quick       1   \n",
       "4        0      0  <|T|> 0       heap       1   \n",
       "\n",
       "                                            text  \n",
       "0     <|Q|> 0 <|S|> bubble <|T|> 0 <|A|> 0 <|E|>  \n",
       "1  <|Q|> 0 <|S|> selection <|T|> 0 <|A|> 0 <|E|>  \n",
       "2  <|Q|> 0 <|S|> insertion <|T|> 0 <|A|> 0 <|E|>  \n",
       "3      <|Q|> 0 <|S|> quick <|T|> 0 <|A|> 0 <|E|>  \n",
       "4       <|Q|> 0 <|S|> heap <|T|> 0 <|A|> 0 <|E|>  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['question'] = question\n",
    "df['answer'] = answer\n",
    "df['solution'] = solution\n",
    "df['algorithm'] = algorithm\n",
    "df['length'] = length\n",
    "df['text'] = text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0550eb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length\n",
       "1         80\n",
       "2        800\n",
       "3       8000\n",
       "4      80000\n",
       "5     800000\n",
       "6    8000000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794c5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142bb802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:40<00:00, 16.73s/it]\n"
     ]
    }
   ],
   "source": [
    "max_len = 6\n",
    "algos = [\"greedy\", \"dp\", \"recursive\", \"brute_force\"]\n",
    "\n",
    "base_path = \"/datadrive/pavan/az_storage/repos/synthetic_reasoning/data/solution_paths/pairs_len\"\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for i in tqdm(range(1, max_len + 1)):\n",
    "    data_dict[i] = {}\n",
    "    for algo in algos:\n",
    "        data_dict[i][algo] = []\n",
    "        path = f\"{base_path}{i}_{algo}.pkl\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data_dict[i][algo] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f44f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy :  1.0, dp :  1.0, recursive :  1.0, brute_force :  2.0, \n",
      "greedy :  1.45, dp :  2.0, recursive :  2.0, brute_force :  2.45, \n",
      "greedy :  2.065, dp :  3.0, recursive :  3.0, brute_force :  4.25, \n",
      "greedy :  2.7625, dp :  3.0, recursive :  4.0, brute_force :  12.3005, \n",
      "greedy :  3.50917, dp :  4.0, recursive :  5.0, brute_force :  54.758, \n",
      "greedy :  4.288345, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp :  4.0, recursive :  6.0, brute_force :  316.302245, \n"
     ]
    }
   ],
   "source": [
    "for l in data_dict:\n",
    "    for algo in data_dict[l]:\n",
    "        # Get the lengths of each inner list\n",
    "        lengths = [len(inner_list) for inner_list in data_dict[l][algo]]\n",
    "        # Calculate the mean length\n",
    "        mean_length = sum(lengths) / len(lengths) if lengths else 0\n",
    "        print(algo, \": \", mean_length, end=\", \")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "86ed768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (l, algo, inner_list[0], inner_list[-1], inner_list[1:-1] if len(inner_list) > 2 else [inner_list[0]])\n",
    "    for l in data_dict\n",
    "    for algo in data_dict[l]\n",
    "    for inner_list in data_dict[l][algo]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cbe7c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"length\", \"algo\", \"question\", \"answer\", \"solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b83b3a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length\n",
       "1         40\n",
       "2        400\n",
       "3       4000\n",
       "4      40000\n",
       "5     400000\n",
       "6    4000000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6f74694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "pretrain       116880\n",
      "rl_finetune    100000\n",
      "val              6000\n",
      "test             6000\n",
      "Name: count, dtype: int64\n",
      "split        length\n",
      "pretrain     1             80\n",
      "             2            800\n",
      "             3           8000\n",
      "             4          35981\n",
      "             5          36025\n",
      "             6          35994\n",
      "rl_finetune  6         100000\n",
      "test         4           2060\n",
      "             5           1949\n",
      "             6           1991\n",
      "val          4           1959\n",
      "             5           2026\n",
      "             6           2015\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and filter by length\n",
    "df4 = df[df['length'] == 4].sample(frac=1, random_state=42)\n",
    "df5 = df[df['length'] == 5].sample(frac=1, random_state=42)\n",
    "df6 = df[df['length'] == 6].sample(frac=1, random_state=42)\n",
    "\n",
    "# --------- Phase 1: Supervised Pretraining ---------\n",
    "\n",
    "# Use equal number from each length (up to 40k)\n",
    "n_per_length = min(len(df4), len(df5), len(df6), 40000)\n",
    "\n",
    "# Sample equal data\n",
    "df4_ = df4.iloc[:n_per_length]\n",
    "df5_ = df5.iloc[:n_per_length]\n",
    "df6_ = df6.iloc[:n_per_length]\n",
    "\n",
    "# Combine and shuffle\n",
    "pretrain_base = pd.concat([df4_, df5_, df6_]).sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Add the lengths 1, 2, 3 to pretrain\n",
    "df123 = df[df['length'].isin([1, 2, 3])].copy()\n",
    "df123[\"split\"] = \"pretrain\"\n",
    "\n",
    "# Split pretrain_base into train/val/test\n",
    "n_total = len(pretrain_base)\n",
    "n_train = int(0.9 * n_total)\n",
    "n_val = int(0.05 * n_total)\n",
    "n_test = n_total - (n_train + n_val)\n",
    "\n",
    "pretrain_df = pretrain_base.iloc[:n_train].copy()\n",
    "val_df = pretrain_base.iloc[n_train:n_train + n_val].copy()\n",
    "test_df = pretrain_base.iloc[n_train + n_val:].copy()\n",
    "\n",
    "pretrain_df[\"split\"] = \"pretrain\"\n",
    "val_df[\"split\"] = \"val\"\n",
    "test_df[\"split\"] = \"test\"\n",
    "\n",
    "# --------- Phase 2: RL Fine-tuning (length=6 only) ---------\n",
    "\n",
    "# Use remaining `length=6` data\n",
    "rl_data = df6.iloc[n_per_length:].copy()\n",
    "rl_finetune_df = rl_data.sample(n=100000, random_state=42).copy()\n",
    "rl_finetune_df[\"split\"] = \"rl_finetune\"\n",
    "\n",
    "# --------- Final Combined Dataset ---------\n",
    "\n",
    "final_df = pd.concat([pretrain_df, val_df, test_df, rl_finetune_df, df123]).reset_index(drop=True)\n",
    "\n",
    "# Sanity checks\n",
    "print(final_df[\"split\"].value_counts())\n",
    "print(final_df.groupby([\"split\", \"length\"]).size())\n",
    "\n",
    "# Optional: save\n",
    "# final_df.to_csv(\"two_phase_split_with_short.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83682f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>algorithm</th>\n",
       "      <th>bubble</th>\n",
       "      <th>cycle</th>\n",
       "      <th>gnome</th>\n",
       "      <th>heap</th>\n",
       "      <th>insertion</th>\n",
       "      <th>quick</th>\n",
       "      <th>selection</th>\n",
       "      <th>shell</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pretrain</th>\n",
       "      <td>14674</td>\n",
       "      <td>14714</td>\n",
       "      <td>14676</td>\n",
       "      <td>14529</td>\n",
       "      <td>14662</td>\n",
       "      <td>14613</td>\n",
       "      <td>14421</td>\n",
       "      <td>14591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rl_finetune</th>\n",
       "      <td>12435</td>\n",
       "      <td>12480</td>\n",
       "      <td>12374</td>\n",
       "      <td>12724</td>\n",
       "      <td>12511</td>\n",
       "      <td>12394</td>\n",
       "      <td>12532</td>\n",
       "      <td>12550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>752</td>\n",
       "      <td>786</td>\n",
       "      <td>767</td>\n",
       "      <td>735</td>\n",
       "      <td>726</td>\n",
       "      <td>734</td>\n",
       "      <td>742</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>753</td>\n",
       "      <td>720</td>\n",
       "      <td>794</td>\n",
       "      <td>785</td>\n",
       "      <td>746</td>\n",
       "      <td>727</td>\n",
       "      <td>707</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "algorithm    bubble  cycle  gnome   heap  insertion  quick  selection  shell\n",
       "split                                                                       \n",
       "pretrain      14674  14714  14676  14529      14662  14613      14421  14591\n",
       "rl_finetune   12435  12480  12374  12724      12511  12394      12532  12550\n",
       "test            752    786    767    735        726    734        742    758\n",
       "val             753    720    794    785        746    727        707    768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group by algorithms and split\n",
    "final_df.groupby(['split', 'algorithm']).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1c9c2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>solution</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 0 7 6 6</td>\n",
       "      <td>0 3 6 6 7</td>\n",
       "      <td>&lt;|T|&gt; 3 0 7 6 6 &lt;|T|&gt; 3 0 6 6 7 &lt;|T|&gt; 0 3 6 6 7</td>\n",
       "      <td>shell</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;|Q|&gt; 3 0 7 6 6 &lt;|S|&gt; shell &lt;|T|&gt; 3 0 7 6 6 &lt;|...</td>\n",
       "      <td>pretrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 3 8 0</td>\n",
       "      <td>0 2 3 8</td>\n",
       "      <td>&lt;|T|&gt; 2 3 8 0 &lt;|T|&gt; 2 2 8 0 &lt;|T|&gt; 2 2 3 0 &lt;|T|...</td>\n",
       "      <td>cycle</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;|Q|&gt; 2 3 8 0 &lt;|S|&gt; cycle &lt;|T|&gt; 2 3 8 0 &lt;|T|&gt; ...</td>\n",
       "      <td>pretrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 5 7 7 1</td>\n",
       "      <td>1 2 5 7 7</td>\n",
       "      <td>&lt;|T|&gt; 2 5 7 7 1 &lt;|T|&gt; 1 2 5 7 7</td>\n",
       "      <td>insertion</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;|Q|&gt; 2 5 7 7 1 &lt;|S|&gt; insertion &lt;|T|&gt; 2 5 7 7 ...</td>\n",
       "      <td>pretrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 4 5 0</td>\n",
       "      <td>0 4 4 5</td>\n",
       "      <td>&lt;|T|&gt; 4 4 5 0 &lt;|T|&gt; 0 4 4 5</td>\n",
       "      <td>insertion</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;|Q|&gt; 4 4 5 0 &lt;|S|&gt; insertion &lt;|T|&gt; 4 4 5 0 &lt;|...</td>\n",
       "      <td>pretrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 4 7 0 3 7</td>\n",
       "      <td>0 3 4 4 7 7</td>\n",
       "      <td>&lt;|T|&gt; 4 4 7 0 3 7 &lt;|T|&gt; 4 4 0 7 3 7 &lt;|T|&gt; 4 0 ...</td>\n",
       "      <td>gnome</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;|Q|&gt; 4 4 7 0 3 7 &lt;|S|&gt; gnome &lt;|T|&gt; 4 4 7 0 3 ...</td>\n",
       "      <td>pretrain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      question       answer  \\\n",
       "0    3 0 7 6 6    0 3 6 6 7   \n",
       "1      2 3 8 0      0 2 3 8   \n",
       "2    2 5 7 7 1    1 2 5 7 7   \n",
       "3      4 4 5 0      0 4 4 5   \n",
       "4  4 4 7 0 3 7  0 3 4 4 7 7   \n",
       "\n",
       "                                            solution  algorithm  length  \\\n",
       "0    <|T|> 3 0 7 6 6 <|T|> 3 0 6 6 7 <|T|> 0 3 6 6 7      shell       5   \n",
       "1  <|T|> 2 3 8 0 <|T|> 2 2 8 0 <|T|> 2 2 3 0 <|T|...      cycle       4   \n",
       "2                    <|T|> 2 5 7 7 1 <|T|> 1 2 5 7 7  insertion       5   \n",
       "3                        <|T|> 4 4 5 0 <|T|> 0 4 4 5  insertion       4   \n",
       "4  <|T|> 4 4 7 0 3 7 <|T|> 4 4 0 7 3 7 <|T|> 4 0 ...      gnome       6   \n",
       "\n",
       "                                                text     split  \n",
       "0  <|Q|> 3 0 7 6 6 <|S|> shell <|T|> 3 0 7 6 6 <|...  pretrain  \n",
       "1  <|Q|> 2 3 8 0 <|S|> cycle <|T|> 2 3 8 0 <|T|> ...  pretrain  \n",
       "2  <|Q|> 2 5 7 7 1 <|S|> insertion <|T|> 2 5 7 7 ...  pretrain  \n",
       "3  <|Q|> 4 4 5 0 <|S|> insertion <|T|> 4 4 5 0 <|...  pretrain  \n",
       "4  <|Q|> 4 4 7 0 3 7 <|S|> gnome <|T|> 4 4 7 0 3 ...  pretrain  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3512331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf448534b0844cc88c14ddf9c4e6b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9155bb5eb24c4988958ba61d691e1eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/117 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1b6b88dfe146b1a30b5135f8f17ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9780b8501aa5493ab8507d712bd47c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01daee174df743a387f9e3c302975125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719aa02f8e924aa6a0a0fc537c4a6a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a13f87fa34a85bc616870804b2a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1713cf87e94f34a8f391d18933ee7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Pavankalyan/syn-reasoning/commit/1160202f288e07c50eb3b9be4bc8d61d76f6ebdc', commit_message='Upload dataset', commit_description='', oid='1160202f288e07c50eb3b9be4bc8d61d76f6ebdc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Pavankalyan/syn-reasoning', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Pavankalyan/syn-reasoning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Group by \"split\" and create DatasetDict\n",
    "split_datasets = {\n",
    "    split: Dataset.from_pandas(sub_df.drop(columns=[\"split\"]))\n",
    "    for split, sub_df in final_df.groupby(\"split\")\n",
    "}\n",
    "\n",
    "dataset_dict = DatasetDict(split_datasets)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "dataset_dict.push_to_hub(\"Pavankalyan/syn-reasoning\", token=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e38d84de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm\n",
       "bubble       28614\n",
       "cycle        28700\n",
       "gnome        28611\n",
       "heap         28773\n",
       "insertion    28645\n",
       "quick        28468\n",
       "selection    28402\n",
       "shell        28667\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['algorithm'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = apply_template(dataset[data_config.train_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32626c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Pavankalyan/sorting-reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfee6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99473848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def process_chunk(chunk_data, algos, data):\n",
    "    start_idx, end_idx = chunk_data\n",
    "    results = []\n",
    "    for i in tqdm(range(start_idx, end_idx)):\n",
    "        q_str = \"<|Q|> \" + ' '.join(map(str, data['ques'][i]))\n",
    "        a_str = \" <|A|> \" + ' '.join(map(str, data['ans'][i])) + \" <|E|>\"\n",
    "        \n",
    "        for alg in algos:\n",
    "            sol_list = data[alg][i]\n",
    "            soln = '<|T|> ' + ' <|T|> '.join(' '.join(map(str, a)) for a in sol_list)\n",
    "            results.append(f\"{q_str} <|S|> {alg} {soln}{a_str}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "314e77bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(n_chunks)\n\u001b[32m      5\u001b[39m chunk_size = \u001b[38;5;28mlen\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mques\u001b[39m\u001b[33m'\u001b[39m]) // n_chunks + \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m chunks = [(i, \u001b[38;5;28mmin\u001b[39m(i + chunk_size, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mques\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data[\u001b[33m'\u001b[39m\u001b[33mques\u001b[39m\u001b[33m'\u001b[39m]), chunk_size)]\n\u001b[32m      7\u001b[39m algos = [\u001b[33m\"\u001b[39m\u001b[33mbubble\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mselection\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minsertion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquick\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mheap\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mshell\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgnome\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcycle\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Calculate total entries for reporting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/datasets/formatting/formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/datasets/formatting/formatting.py:408\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_row(pa_table)\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_batch(pa_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/datasets/formatting/formatting.py:459\u001b[39m, in \u001b[36mPythonFormatter.format_column\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     column = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     column = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_column(column, pa_table.column_names[\u001b[32m0\u001b[39m])\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/site-packages/datasets/formatting/formatting.py:146\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_column\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data = dataset[\"train\"]\n",
    "# Split the work into chunks based on CPU count\n",
    "n_chunks = os.cpu_count()\n",
    "print(n_chunks)\n",
    "chunk_size = len(data['ques']) // n_chunks + 1\n",
    "chunks = [(i, min(i + chunk_size, len(data['ques']))) for i in range(0, len(data['ques']), chunk_size)]\n",
    "algos = [\"bubble\", \"selection\", \"insertion\", \"quick\", \"heap\", \"shell\", \"gnome\", \"cycle\"]\n",
    "# Calculate total entries for reporting\n",
    "total_entries = len(data['ques']) * len(algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591eb950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in parallel with progress tracking\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    func = functools.partial(process_chunk, algos=algos, data=data)\n",
    "    results = list(tqdm(executor.map(func, chunks), \n",
    "                        total=len(chunks),\n",
    "                        desc=\"Processing chunks\",\n",
    "                        bar_format=\"{desc}: {n_fmt}/{total_fmt} chunks [{bar}] {percentage:3.0f}% | ~{elapsed}<{remaining}\"))\n",
    "\n",
    "# Flatten results\n",
    "texts = [item for sublist in results for item in sublist]\n",
    "\n",
    "print(f\"Generated {len(texts)}/{total_entries} text entries (100% complete)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9653d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_chunk(chunk_data, algos, data):\n",
    "    \"\"\"Process a chunk of data more efficiently\"\"\"\n",
    "    start_idx, end_idx = chunk_data\n",
    "    \n",
    "    # Calculate expected output size and preallocate\n",
    "    n_entries = (end_idx - start_idx) * len(algos)\n",
    "    results = [None] * n_entries\n",
    "    \n",
    "    # Track chunk processing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    idx = 0\n",
    "    # Process each question in this chunk\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Create question string once for this index\n",
    "        q_str = \"<|Q|> \" + ' '.join(map(str, data['ques'][i]))\n",
    "        a_str = \" <|A|> \" + ' '.join(map(str, data['ans'][i])) + \" <|E|>\"\n",
    "        \n",
    "        # Process each algorithm for this question\n",
    "        for alg in algos:\n",
    "            # Get solution list for this algorithm and question\n",
    "            sol_list = data[alg][i]\n",
    "            \n",
    "            # Optimize solution string creation\n",
    "            # Using list comprehension inside join is faster than nested joins\n",
    "            soln_parts = []\n",
    "            for solution in sol_list:\n",
    "                soln_parts.append(' '.join(map(str, solution)))\n",
    "            \n",
    "            soln = '<|T|> ' + ' <|T|> '.join(soln_parts)\n",
    "            \n",
    "            # Store result directly into preallocated list\n",
    "            results[idx] = f\"{q_str} <|S|> {alg} {soln}{a_str}\"\n",
    "            idx += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    \n",
    "    # Return results and some stats about this chunk\n",
    "    return {\n",
    "        'results': results,\n",
    "        'chunk_size': end_idx - start_idx,\n",
    "        'process_time': process_time,\n",
    "        'entries_created': idx\n",
    "    }\n",
    "\n",
    "def optimize_processing(dataset, split=\"train\", max_workers=None):\n",
    "    \"\"\"Main optimization function to process dataset efficiently\"\"\"\n",
    "    data = dataset[split]\n",
    "    data_size = len(data['ques'])\n",
    "    \n",
    "    # Get CPU count, limit if needed\n",
    "    cpu_count = os.cpu_count()\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, cpu_count - 1)  # Leave one CPU free for system\n",
    "    else:\n",
    "        max_workers = min(max_workers, cpu_count)\n",
    "    \n",
    "    print(f\"Using {max_workers} workers out of {cpu_count} available CPUs\")\n",
    "    \n",
    "    # Calculate optimal chunk size based on data and CPU count\n",
    "    # Aim for ~4-8 chunks per CPU for good balancing\n",
    "    target_chunks = max_workers * 4\n",
    "    chunk_size = max(1, data_size // target_chunks)\n",
    "    chunks = []\n",
    "    \n",
    "    # Create chunks with progress reporting\n",
    "    for i in range(0, data_size, chunk_size):\n",
    "        chunks.append((i, min(i + chunk_size, data_size)))\n",
    "    \n",
    "    # Define algorithms\n",
    "    algos = [\"bubble\", \"selection\", \"insertion\", \"quick\", \"heap\", \"shell\", \"gnome\", \"cycle\"]\n",
    "    total_entries = data_size * len(algos)\n",
    "    \n",
    "    print(f\"Processing {data_size} questions with {len(algos)} algorithms = {total_entries} entries\")\n",
    "    print(f\"Divided into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Process in parallel with clear progress tracking\n",
    "    start_time = time.time()\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        func = functools.partial(process_chunk, algos=algos, data=data)\n",
    "        \n",
    "        # Use tqdm to show progress\n",
    "        results = list(tqdm(\n",
    "            executor.map(func, chunks),\n",
    "            total=len(chunks),\n",
    "            desc=f\"Processing {split} data\",\n",
    "            bar_format=\"{desc}: {n_fmt}/{total_fmt} chunks [{bar}] {percentage:3.0f}% | {elapsed}<{remaining}\"\n",
    "        ))\n",
    "    \n",
    "    # Flatten results and gather stats\n",
    "    texts = []\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    for result in results:\n",
    "        texts.extend(result['results'])\n",
    "        total_processing_time += result['process_time']\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    end_time = time.time()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"- Wall time: {wall_time:.2f} seconds\")\n",
    "    print(f\"- CPU processing time: {total_processing_time:.2f} seconds\")\n",
    "    print(f\"- Efficiency: {total_processing_time/(wall_time*max_workers)*100:.1f}% CPU utilization\")\n",
    "    print(f\"- Processing rate: {len(texts)/wall_time:.1f} entries/second\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Usage:\n",
    "# texts = optimize_processing(dataset, split=\"train\")\n",
    "# Or with custom worker count: \n",
    "# texts = optimize_processing(dataset, split=\"train\", max_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 95 workers out of 96 available CPUs\n",
      "Processing 581110 questions with 8 algorithms = 4648880 entries\n",
      "Divided into 381 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 0/381 chunks [          ]   0% | 04:29<?\n"
     ]
    }
   ],
   "source": [
    "texts = optimize_processing(dataset, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bd9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def process_chunk(chunk_data, algos, data):\n",
    "    \"\"\"Process a chunk of data more efficiently\"\"\"\n",
    "    start_idx, end_idx = chunk_data\n",
    "    \n",
    "    # Calculate expected output size and preallocate\n",
    "    n_entries = (end_idx - start_idx) * len(algos)\n",
    "    results = [None] * n_entries\n",
    "    \n",
    "    # Track chunk processing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    idx = 0\n",
    "    # Process each question in this chunk\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Create question string once for this index\n",
    "        q_str = \"<|Q|> \" + ' '.join(map(str, data['ques'][i]))\n",
    "        a_str = \" <|A|> \" + ' '.join(map(str, data['ans'][i])) + \" <|E|>\"\n",
    "        \n",
    "        # Process each algorithm for this question\n",
    "        for alg in algos:\n",
    "            # Get solution list for this algorithm and question\n",
    "            sol_list = data[alg][i]\n",
    "            \n",
    "            # Optimize solution string creation\n",
    "            # Using list comprehension inside join is faster than nested joins\n",
    "            soln_parts = []\n",
    "            for solution in sol_list:\n",
    "                soln_parts.append(' '.join(map(str, solution)))\n",
    "            \n",
    "            soln = '<|T|> ' + ' <|T|> '.join(soln_parts)\n",
    "            \n",
    "            # Store result directly into preallocated list\n",
    "            results[idx] = f\"{q_str} <|S|> {alg} {soln}{a_str}\"\n",
    "            idx += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    \n",
    "    # Return results and some stats about this chunk\n",
    "    return {\n",
    "        'results': results,\n",
    "        'chunk_size': end_idx - start_idx,\n",
    "        'process_time': process_time,\n",
    "        'entries_created': idx\n",
    "    }\n",
    "\n",
    "def optimize_processing(dataset, split=\"train\", max_workers=None, chunk_size=None):\n",
    "    \"\"\"Main optimization function to process dataset efficiently\"\"\"\n",
    "    data = dataset[split]\n",
    "    data_size = len(data['ques'])\n",
    "    \n",
    "    # Get CPU count, limit if needed\n",
    "    cpu_count = os.cpu_count()\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, cpu_count - 1)  # Leave one CPU free for system\n",
    "    else:\n",
    "        max_workers = min(max_workers, cpu_count)\n",
    "    \n",
    "    print(f\"Using {max_workers} workers out of {cpu_count} available CPUs\")\n",
    "    \n",
    "    # Calculate optimal chunk size based on data and CPU count\n",
    "    if chunk_size is None:\n",
    "        # Aim for ~4-8 chunks per CPU for good balancing\n",
    "        target_chunks = max_workers * 4\n",
    "        chunk_size = max(1, data_size // target_chunks)\n",
    "    \n",
    "    # Create a smaller number of larger chunks for better progress visibility\n",
    "    chunks = []\n",
    "    for i in range(0, data_size, chunk_size):\n",
    "        chunks.append((i, min(i + chunk_size, data_size)))\n",
    "    \n",
    "    # Define algorithms\n",
    "    algos = [\"bubble\", \"selection\", \"insertion\", \"quick\", \"heap\", \"shell\", \"gnome\", \"cycle\"]\n",
    "    total_entries = data_size * len(algos)\n",
    "    \n",
    "    print(f\"Processing {data_size} questions with {len(algos)} algorithms = {total_entries} entries\")\n",
    "    print(f\"Divided into {len(chunks)} chunks of ~{chunk_size} questions each\")\n",
    "    \n",
    "    # Create a manager for shared progress tracking\n",
    "    manager = multiprocessing.Manager()\n",
    "    counter = manager.Value('i', 0)\n",
    "    lock = manager.Lock()\n",
    "    \n",
    "    # Create a wrapper function that updates the counter\n",
    "    def process_chunk_with_progress(chunk_data):\n",
    "        result = process_chunk(chunk_data, algos=algos, data=data)\n",
    "        with lock:\n",
    "            counter.value += 1\n",
    "        return result\n",
    "    \n",
    "    # Process in parallel with clear progress tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create the progress bar outside the executor\n",
    "    pbar = tqdm(total=len(chunks), desc=f\"Processing {split} data\", \n",
    "                bar_format=\"{desc}: {n_fmt}/{total_fmt} chunks [{bar}] {percentage:3.0f}% | {elapsed}<{remaining}\")\n",
    "    \n",
    "    # Start the executor\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all jobs\n",
    "        future_to_chunk = {executor.submit(process_chunk_with_progress, chunk): chunk for chunk in chunks}\n",
    "        \n",
    "        # Monitor progress and collect results\n",
    "        last_count = 0\n",
    "        while len(results) < len(chunks):\n",
    "            # Update progress bar based on shared counter\n",
    "            current_count = counter.value\n",
    "            if current_count > last_count:\n",
    "                pbar.update(current_count - last_count)\n",
    "                last_count = current_count\n",
    "            \n",
    "            # Check for completed futures\n",
    "            done_futures = [f for f in future_to_chunk.keys() if f.done()]\n",
    "            for future in done_futures:\n",
    "                try:\n",
    "                    results.append(future.result())\n",
    "                    del future_to_chunk[future]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing chunk: {e}\")\n",
    "            \n",
    "            # Short sleep to avoid CPU spinning\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Flatten results and gather stats\n",
    "    texts = []\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    for result in results:\n",
    "        texts.extend(result['results'])\n",
    "        total_processing_time += result['process_time']\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    end_time = time.time()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"- Wall time: {wall_time:.2f} seconds\")\n",
    "    print(f\"- CPU processing time: {total_processing_time:.2f} seconds\")\n",
    "    print(f\"- Efficiency: {total_processing_time/(wall_time*max_workers)*100:.1f}% CPU utilization\")\n",
    "    print(f\"- Processing rate: {len(texts)/wall_time:.1f} entries/second\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Usage example for smaller chunk size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16357834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def process_chunk(chunk_data, algos, data):\n",
    "    \"\"\"Process a chunk of data more efficiently\"\"\"\n",
    "    start_idx, end_idx = chunk_data\n",
    "    \n",
    "    # Calculate expected output size and preallocate\n",
    "    n_entries = (end_idx - start_idx) * len(algos)\n",
    "    results = [None] * n_entries\n",
    "    \n",
    "    # Track chunk processing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    idx = 0\n",
    "    # Process each question in this chunk\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Create question string once for this index\n",
    "        q_str = \"<|Q|> \" + ' '.join(map(str, data['ques'][i]))\n",
    "        a_str = \" <|A|> \" + ' '.join(map(str, data['ans'][i])) + \" <|E|>\"\n",
    "        \n",
    "        # Process each algorithm for this question\n",
    "        for alg in algos:\n",
    "            # Get solution list for this algorithm and question\n",
    "            sol_list = data[alg][i]\n",
    "            \n",
    "            # Optimize solution string creation\n",
    "            soln_parts = [' '.join(map(str, solution)) for solution in sol_list]\n",
    "            soln = '<|T|> ' + ' <|T|> '.join(soln_parts)\n",
    "            \n",
    "            # Store result directly into preallocated list\n",
    "            results[idx] = f\"{q_str} <|S|> {alg} {soln}{a_str}\"\n",
    "            idx += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'chunk_size': end_idx - start_idx,\n",
    "        'process_time': process_time,\n",
    "        'entries_created': idx\n",
    "    }\n",
    "\n",
    "def process_chunk_with_progress(chunk_data, algos, data, counter, lock):\n",
    "    \"\"\"Wrapper function to process a chunk and update progress\"\"\"\n",
    "    result = process_chunk(chunk_data, algos, data)\n",
    "    with lock:\n",
    "        counter.value += 1\n",
    "    return result\n",
    "\n",
    "def optimize_processing(dataset, split=\"train\", max_workers=None, chunk_size=None):\n",
    "    \"\"\"Main optimization function to process dataset efficiently\"\"\"\n",
    "    data = dataset[split].select(range(10000))\n",
    "    data_size = len(data['ques'])\n",
    "    \n",
    "    # Get CPU count, limit if needed\n",
    "    cpu_count = os.cpu_count()\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, cpu_count - 1)  # Leave one CPU free for system\n",
    "    else:\n",
    "        max_workers = min(max_workers, cpu_count)\n",
    "    \n",
    "    print(f\"Using {max_workers} workers out of {cpu_count} available CPUs\")\n",
    "    \n",
    "    # Calculate optimal chunk size based on data and CPU count\n",
    "    if chunk_size is None:\n",
    "        target_chunks = max_workers * 4\n",
    "        chunk_size = max(1, data_size // target_chunks)\n",
    "    \n",
    "    chunks = [(i, min(i + chunk_size, data_size)) for i in range(0, data_size, chunk_size)]\n",
    "    \n",
    "    algos = [\"bubble\", \"selection\", \"insertion\", \"quick\", \"heap\", \"shell\", \"gnome\", \"cycle\"]\n",
    "    total_entries = data_size * len(algos)\n",
    "    \n",
    "    print(f\"Processing {data_size} questions with {len(algos)} algorithms = {total_entries} entries\")\n",
    "    print(f\"Divided into {len(chunks)} chunks of ~{chunk_size} questions each\")\n",
    "    \n",
    "    manager = multiprocessing.Manager()\n",
    "    counter = manager.Value('i', 0)\n",
    "    lock = manager.Lock()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(total=len(chunks), desc=f\"Processing {split} data\", \n",
    "                bar_format=\"{desc}: {n_fmt}/{total_fmt} chunks [{bar}] {percentage:3.0f}% | {elapsed}<{remaining}\")\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_chunk = {\n",
    "            executor.submit(process_chunk_with_progress, chunk, algos, data, counter, lock): chunk\n",
    "            for chunk in chunks\n",
    "        }\n",
    "        \n",
    "        last_count = 0\n",
    "        while len(results) < len(chunks):\n",
    "            current_count = counter.value\n",
    "            if current_count > last_count:\n",
    "                pbar.update(current_count - last_count)\n",
    "                last_count = current_count\n",
    "            \n",
    "            done_futures = [f for f in future_to_chunk.keys() if f.done()]\n",
    "            for future in done_futures:\n",
    "                try:\n",
    "                    results.append(future.result())\n",
    "                    del future_to_chunk[future]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing chunk: {e}\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    texts = []\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    for result in results:\n",
    "        texts.extend(result['results'])\n",
    "        total_processing_time += result['process_time']\n",
    "    \n",
    "    end_time = time.time()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"- Wall time: {wall_time:.2f} seconds\")\n",
    "    print(f\"- CPU processing time: {total_processing_time:.2f} seconds\")\n",
    "    print(f\"- Efficiency: {total_processing_time/(wall_time*max_workers)*100:.1f}% CPU utilization\")\n",
    "    print(f\"- Processing rate: {len(texts)/wall_time:.1f} entries/second\")\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def process_chunk(chunk_data, algos, data):\n",
    "    \"\"\"Process a chunk of data more efficiently\"\"\"\n",
    "    start_idx, end_idx = chunk_data\n",
    "    \n",
    "    # Calculate expected output size and preallocate\n",
    "    n_entries = (end_idx - start_idx) * len(algos)\n",
    "    results = [None] * n_entries\n",
    "    \n",
    "    # Track chunk processing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    idx = 0\n",
    "    # Process each question in this chunk\n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Create question string once for this index\n",
    "        q_str = \"<|Q|> \" + ' '.join(map(str, data['ques'][i]))\n",
    "        a_str = \" <|A|> \" + ' '.join(map(str, data['ans'][i])) + \" <|E|>\"\n",
    "        \n",
    "        # Process each algorithm for this question\n",
    "        for alg in algos:\n",
    "            # Get solution list for this algorithm and question\n",
    "            sol_list = data[alg][i]\n",
    "            \n",
    "            # Optimize solution string creation\n",
    "            soln_parts = [' '.join(map(str, solution)) for solution in sol_list]\n",
    "            soln = '<|T|> ' + ' <|T|> '.join(soln_parts)\n",
    "            \n",
    "            # Store result directly into preallocated list\n",
    "            results[idx] = f\"{q_str} <|S|> {alg} {soln}{a_str}\"\n",
    "            idx += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    process_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'chunk_size': end_idx - start_idx,\n",
    "        'process_time': process_time,\n",
    "        'entries_created': idx\n",
    "    }\n",
    "\n",
    "def process_chunk_with_progress(chunk_data, algos, data, counter, lock):\n",
    "    \"\"\"Wrapper function to process a chunk and update progress\"\"\"\n",
    "    result = process_chunk(chunk_data, algos, data)\n",
    "    with lock:\n",
    "        counter.value += 1\n",
    "    return result\n",
    "\n",
    "def optimize_processing(dataset, split=\"train\", max_workers=None, chunk_size=None):\n",
    "    \"\"\"Main optimization function to process dataset efficiently\"\"\"\n",
    "    data = dataset[split]\n",
    "    data_size = len(data['ques'])\n",
    "    \n",
    "    # Get CPU count, limit if needed\n",
    "    cpu_count = os.cpu_count()\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, cpu_count - 1)  # Leave one CPU free for system\n",
    "    else:\n",
    "        max_workers = min(max_workers, cpu_count)\n",
    "    \n",
    "    print(f\"Using {max_workers} workers out of {cpu_count} available CPUs\")\n",
    "    \n",
    "    # Calculate optimal chunk size based on data and CPU count\n",
    "    if chunk_size is None:\n",
    "        target_chunks = max_workers * 4\n",
    "        chunk_size = max(1, data_size // target_chunks)\n",
    "    \n",
    "    chunks = [(i, min(i + chunk_size, data_size)) for i in range(0, data_size, chunk_size)]\n",
    "    \n",
    "    algos = [\"bubble\", \"selection\", \"insertion\", \"quick\", \"heap\", \"shell\", \"gnome\", \"cycle\"]\n",
    "    total_entries = data_size * len(algos)\n",
    "    \n",
    "    print(f\"Processing {data_size} questions with {len(algos)} algorithms = {total_entries} entries\")\n",
    "    print(f\"Divided into {len(chunks)} chunks of ~{chunk_size} questions each\")\n",
    "    \n",
    "    manager = multiprocessing.Manager()\n",
    "    counter = manager.Value('i', 0)\n",
    "    lock = manager.Lock()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(total=len(chunks), desc=f\"Processing {split} data\", \n",
    "                bar_format=\"{desc}: {n_fmt}/{total_fmt} chunks [{bar}] {percentage:3.0f}% | {elapsed}<{remaining}\")\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_chunk = {\n",
    "            executor.submit(process_chunk_with_progress, chunk, algos, data, counter, lock): chunk\n",
    "            for chunk in chunks\n",
    "        }\n",
    "        \n",
    "        last_count = 0\n",
    "        while len(results) < len(chunks):\n",
    "            current_count = counter.value\n",
    "            if current_count > last_count:\n",
    "                pbar.update(current_count - last_count)\n",
    "                last_count = current_count\n",
    "            \n",
    "            done_futures = [f for f in future_to_chunk.keys() if f.done()]\n",
    "            for future in done_futures:\n",
    "                try:\n",
    "                    results.append(future.result())\n",
    "                    del future_to_chunk[future]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing chunk: {e}\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    texts = []\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    for result in results:\n",
    "        texts.extend(result['results'])\n",
    "        total_processing_time += result['process_time']\n",
    "    \n",
    "    end_time = time.time()\n",
    "    wall_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"- Wall time: {wall_time:.2f} seconds\")\n",
    "    print(f\"- CPU processing time: {total_processing_time:.2f} seconds\")\n",
    "    print(f\"- Efficiency: {total_processing_time/(wall_time*max_workers)*100:.1f}% CPU utilization\")\n",
    "    print(f\"- Processing rate: {len(texts)/wall_time:.1f} entries/second\")\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf5db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 workers out of 96 available CPUs\n",
      "Processing 10000 questions with 8 algorithms = 80000 entries\n",
      "Divided into 5 chunks of ~2000 questions each\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 0/5 chunks [          ]   0% | 00:00<?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-18:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-20:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-9:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-24:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-15:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-8:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-21:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "Process ForkProcess-12:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-11:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-7:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-14:\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/concurrent/futures/process.py\", line 252, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/datadrive/pavan/anaconda3/envs/gptmin/lib/python3.12/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m texts = \u001b[43moptimize_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36moptimize_processing\u001b[39m\u001b[34m(dataset, split, max_workers, chunk_size)\u001b[39m\n\u001b[32m    111\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    112\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing chunk: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m pbar.close()\n\u001b[32m    118\u001b[39m texts = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "texts = optimize_processing(dataset, split=\"train\", max_workers=24, chunk_size=2000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
